\documentclass[12pt,preprint]{aastex}

\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}

\usepackage{url}
\usepackage{amssymb,amsmath}
\usepackage{graphicx}

\newcommand{\arxiv}[1]{\href{http://arxiv.org/abs/#1}{arXiv:#1}}

\newcommand{\project}[1]{{\sffamily #1}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}
\newcommand{\documentname}{\textsl{Note}}

\newcommand{\Fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\fig}[1]{\Fig{#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\Tab}[1]{Table~\ref{tab:#1}}
\newcommand{\tab}[1]{\Tab{#1}}
\newcommand{\tablabel}[1]{\label{tab:#1}}
\newcommand{\Eq}[1]{Equation~(\ref{eq:#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}
\newcommand{\Sect}[1]{Section~\ref{sect:#1}}
\newcommand{\sect}[1]{\Sect{#1}}
\newcommand{\App}[1]{Appendix~\ref{sect:#1}}
\newcommand{\app}[1]{\App{#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}

\newcommand{\dd}{\,\mathrm{d}}
\newcommand{\pr}[1]{p ( #1 )}
\newcommand{\bvec}[1]{\boldsymbol{#1}}
\newcommand{\unit}[1]{\,\mathrm{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{A note on hierarchical inference \& the distances to K giants}

\author{%
    Daniel~Foreman-Mackey\altaffilmark{\ref{CCPP},\ref{email}}, %
    David~W.~Hogg\altaffilmark{\ref{CCPP},\ref{MPIA}}, %
    \etal%
}

\newcounter{address}
\setcounter{address}{1}
\altaffiltext{\theaddress}{\stepcounter{address}\label{CCPP} Center
  for Cosmology and Particle Physics, Department of Physics, New York
  University, 4 Washington Place, New York, NY 10003}
\altaffiltext{\theaddress}{\stepcounter{address}\label{email} To whom
  correspondence should be addressed: \texttt{danfm@nyu.edu}}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIA}
  Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117
  Heidelberg, Germany}

\begin{abstract}
\end{abstract}

\keywords{%
    methods: data analysis ---
    methods: statistical
}

\clearpage

\section{The General Hierarchical Problem}

To start, we will start by demonstrating a general hierarchical inference
on the graphical model shown in \fig{gm-simple}. The factorization of the
joint distribution implied by this model is
\begin{equation}
    \pr{\alpha, \{ w_n \}, \{ x_n \}} =
        \pr{\alpha} \, \prod_{n=1}^N \pr{w_n | \alpha} \, \pr{x_n | w_n}
        \quad.
\end{equation}
The physical interpretation of this model is something like: $\alpha$ is the
\emph{hyperparameter} (or set of hyperparameters) that parameterize the
physical theory of interest and $x \equiv \{ x_n \}$ is the set of
observations---or objects (we will use both interchangeably---that will be
used to constrain the model. DFM: talk a little bit more about what $x_n$ can
be\ldots this will help with the generality argument. Each $x_n$ can (of
course) be a vector or set of different observations (including experimental
uncertainties). The parameter $w_n$ is called a \emph{latent variable} (or
variables). The structure of the model asserts that the \emph{distribution}
from which $x_n$ is drawn is fully parameterized by the setting of $w_n$.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.2\textwidth]{gm/gm-simple.pdf}
    \end{center}
    \caption{A simple probabilistic model.\figlabel{gm-simple}}
\end{figure}

In many scientific problems, the $w_n$ are the ``model parameters'' that are
learned by sampling or optimizing the posterior distribution $\pr{w_n | x_n}$
for a particular observation $n$. The \emph{hierarchical} framework
is used when a particular $w_n$---for example, the orbital parameters of
\emph{one specific} exoplanet or the distance, luminosity and metallicity of
a single K giant---are not (by themselves) an interesting scientific result.
Instead, hierarchical inference is more concerned with learning the
\emph{distribution} of physical parameters across an ensemble of
objects---the parameters $\alpha$ in this example.

A little bit about why histograms are The Wrong Thing\ldots

While this model seems very simplistic at first, the structure is actually
very general. This is because both $\alpha$ and $w$ can have an arbitrarily
large number of dimensions and the distribution $\pr{x_n | w_n}$ can be
extremely complicated. In fact, under the assumption that the different
observations (or objects) are independent, a huge class of exciting
scientific research questions can be posed in terms of a hierarchical
inference.

Discuss why the maximum likelihood values of $\alpha$ are what we want\ldots

The standard result of a probabilistic analysis is (an approximation of) the
posterior density function
\begin{equation}
    \pr{w_n | x_n, \alpha} \propto \pr{w_n | \alpha} \, \pr{x_n | w_n, \alpha}
\end{equation}
where $\pr{w_n | \alpha}$ is the prior on the parameters $w_n$ that is chosen
by the researcher. The goal of hierarchical inference is to infer the
maximum marginalized likelihood estimate (MMLE) of the parameters $\alpha$
that is consistent with all the data $x \equiv \{ x_n \}$
\begin{equation}
    \alpha_\mathrm{ML} = \argmax_\alpha \pr{x | \alpha}
\end{equation}
or equivalently
\begin{equation} \eqlabel{max-like}
    \alpha_\mathrm{ML} = \argmax_\alpha \log \pr{x | \alpha}
        = \argmax_\alpha \ell (\alpha)
\end{equation}
where $\ell (\alpha)$ is the log-marginalized-likelihood. MMLE is called
``marginalized'' because
\begin{equation} \eqlabel{marginalized-like}
    \pr{x | \alpha} = \int \pr{x, w | \alpha} \dd w
        = \int \prod_{n = 1} ^N  \pr{x_n, w_n | \alpha} \dd w_n
        = \prod_{n = 1} ^N \left [ \int \pr{x_n, w_n | \alpha} \dd w_n
            \right ]
\end{equation}
involves a marginalization over the specific latent parameters of observation
$n$. In practice, this marginalization is not generally tractable
analytically and it will be done using costly numerical procedures like
Markov chain Monte Carlo (MCMC). Furthermore, the maximization in
\eq{max-like} must also be performed numerically using an algorithm like
gradient descent or expectation-maximization (EM). This becomes quickly
intractable because each evaluation of $\ell (\alpha)$ (or its gradient)
involves the expensive marginalization from \eq{marginalized-like}.

\paragraph{Expectation-maximization and importance sampling}
A common ``dataset'' used for hierarchical inference is a set of $J$ samples
\begin{equation}
    w^{(j)} = \{ w_n ^{(j)} \}
\end{equation}
drawn from the posterior distribution
\begin{equation}
    \pr{w_n | x_n, \alpha^{(0)}}
        \propto \pr{w_n | \alpha^{(0)}} \, \pr{x_n | w_n, \alpha^{(0)}}
\end{equation}
and a functional form---either informative or uninformative---for the prior
density that was chosen by some other experimenter or team
\begin{equation}
    \pr{w_n | \alpha^{(0)}} \quad.
\end{equation}
Using these samples, we would like to infer the MMLE values for $\alpha$
that is consistent with the entire dataset using only the samples that
we are given. The key insight here is that \eq{marginalized-like} can be
written
\begin{eqnarray}
    \pr{x | \alpha}
        &=& \prod_{n = 1} ^N \int \pr{x_n, w_n | \alpha} \dd w_n \\
        &=& \prod_{n = 1} ^N \int \pr{x_n | w_n} \, \pr{w_n | \alpha} \dd w_n
    \quad.
\end{eqnarray}
Because of the factorization implied by \fig{gm-simple}, this can be
rewritten as
\begin{eqnarray}
    \pr{x | \alpha}
        &=& \prod_{n = 1} ^N \int
            \left [ \frac{\pr{x_n | \alpha^{(0)}} \,
                \pr{w_n | x_n, \alpha^{(0)}}}{\pr{w_n | \alpha^{(0)}}}  \right ]
            \, \pr{w_n | \alpha} \dd w_n \quad.
\end{eqnarray}
Then, we can pull the term $\pr{x_n | \alpha^{(0)}}$ out of the integral
\begin{eqnarray}
    \pr{x | \alpha} &=& \prod_{n = 1}^N \pr{x_n | \alpha^{(0)}}
        \int \pr{w_n | x_n, \alpha^{(0)}}
        \, \frac{\pr{w_n | \alpha}}{\pr{w_n | \alpha^{(0)}}} \dd w_n \\
    &=& Z(x, \alpha^{(0)}) \, \prod_{n = 1}^N
        \int \pr{w_n | x_n, \alpha^{(0)}}
        \, \frac{\pr{w_n | \alpha}}{\pr{w_n | \alpha^{(0)}}} \dd w_n
\end{eqnarray}
where $Z(x, \alpha^{(0)})$ is called the \emph{evidence} and for our
purposes---the maximization in \eq{max-like}---it is just a constant.
Finally, using the samples $w^{(j)}$, we get
\begin{equation}
    \pr{x | \alpha} \approx Z(x, \alpha^{(0)}) \, \prod_{n = 1}^N
        \frac{1}{J} \sum_{j=1}^{J}
        \frac{\pr{w_n^{(j)} | \alpha}}{\pr{w_n^{(j)} | \alpha^{(0)}}} \quad.
\end{equation}
Substituting this into \eq{max-like}, we find
\begin{equation}
    \alpha_\mathrm{ML} \approx \argmax_\alpha \sum_{n=1}^N \log \sum_{j=1}^{J}
        \frac{\pr{w_n^{(j)} | \alpha}}{\pr{w_n^{(j)} | \alpha^{(0)}}}
\end{equation}
where the term inside the inner sum is just the importance sampled
marginalized likelihoods of the $w_n^{(j)}$ given a proposal $\alpha$.

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.6\textwidth]{gm/gm-hw.pdf}
    \end{center}
    \caption{K giant PGM.\figlabel{gm-hw}}
\end{figure}



% \acknowledgements It is a pleasure to thank\ldots

% \begin{thebibliography}{}\raggedright

% \end{thebibliography}

\end{document}
